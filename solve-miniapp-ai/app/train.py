import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW 
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Config) ---
MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
DATA_FILE = 'urgency_dataset.csv'
OUTPUT_DIR = 'my_custom_model'
EPOCHS = 10   #‡πÄ‡∏ó‡∏£‡∏ô 10 ‡∏£‡∏≠‡∏ö
BATCH_SIZE = 10 #‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ 10          
LEARNING_RATE = 2e-5    

# --- 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Dataset Class) ---
class UrgencyDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            # (FIX) ‡πÅ‡∏õ‡∏•‡∏á label ‡πÄ‡∏õ‡πá‡∏ô int ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ torch
            'labels': torch.tensor(int(label), dtype=torch.long)
        }

# --- 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ó‡∏£‡∏ô (Training Function) ---
def train():
    print(f"Starting Training with model: {MODEL_NAME}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Training on: {device}")

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    try:
        df = pd.read_csv(DATA_FILE)
        # (FIX) ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î)
        df['label'] = pd.to_numeric(df['label'], errors='coerce').fillna(0).astype(int)
        print(f"Loaded {len(df)} examples from {DATA_FILE}")
    except FileNotFoundError:
        print("Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå dataset! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå urgency_dataset.csv ‡∏Å‡πà‡∏≠‡∏ô")
        return

    # ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô
    tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{MODEL_NAME}')
    model = AutoModelForSequenceClassification.from_pretrained(
        f'sentence-transformers/{MODEL_NAME}',
        num_labels=3 # 0, 1, 2
    )
    model.to(device)

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    dataset = UrgencyDataset(df.text.to_numpy(), df.label.to_numpy(), tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        print(f"\nüéì Epoch {epoch + 1}/{EPOCHS}")
        
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Average Loss: {avg_loss:.4f}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    print("\nSaving model...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f" Done! Model saved to folder: '{OUTPUT_DIR}'")

if __name__ == "__main__":
    train()